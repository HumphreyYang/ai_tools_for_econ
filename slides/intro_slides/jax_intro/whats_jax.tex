\input{../../preamble}

\title{Google JAX}



\author{John Stachurski}


\date{2025}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}



\begin{frame}
    \frametitle{Topics}

    \begin{itemize}
        \item Foo
        \vspace{0.5em}
        \item Bar
        \vspace{0.5em}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{What's JAX?}

    \url{https://jax.readthedocs.io/en/latest/}
    
            \vspace{0.5em}
    \begin{itemize}
        \item \brown{J}ust-in-time compilation
            \vspace{0.5em}
        \item \brown{A}utomatic differentiation
            \vspace{0.5em}
        \item \brown{X}ccelerated linear algebra
    \end{itemize}

            \vspace{0.5em}
            \vspace{0.5em}

\end{frame}

\begin{frame}
    
    \Eg AlphaFold3 (Google JAX)

        \vspace{0.5em}
    \textbf{Highly accurate protein structure prediction with AlphaFold}

        \vspace{0.5em}
    John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
    Olaf Ronneberger, Kathryn Tunyasuvunakool,\ldots 

        \vspace{0.5em}
    \underline{Nature} Vol.\ 596 (2021)

    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \begin{itemize}
        \item Citation count $= ~30$K
        \vspace{0.5em}
        \item Nobel Prize in Chemistry 2024
    \end{itemize}

\end{frame}



\begin{frame}
    \frametitle{History: Setting the stage}

    \begin{itemize}
        \item Some history of scientific computing
        \item Dynamic and static types
        \item Background on vectorization / JIT compilers
    \end{itemize}

\end{frame}
    


\begin{frame}
    \frametitle{Fortran / C  --- static types and AOT compilers}


    \Eg Suppose we want to compute the sequence
    %
    \begin{equation*}
        k_{t+1} = s k_t^\alpha + (1 - \delta) k_t
    \end{equation*}
    %
    from some given $k_0$ 

        \vspace{0.5em}
        \vspace{0.5em}
        \vspace{0.5em}

    Let's write a function in C that 
    %
    \begin{enumerate}
        \item implements the loop 
        \vspace{0.5em}
        \item returns the last $k_t$
    \end{enumerate}


\end{frame}

\begin{frame}[fragile]
    
    \begin{minted}{c}
#include <stdio.h>
#include <math.h>

int main() {
    double k = 0.2;
    double alpha = 0.4;
    double s = 0.3;
    double delta = 0.1;
    int i;
    int n = 1000;
    for (i = 0; i < n; i++) {
        k = s * pow(k, alpha) + (1 - delta) * k;
    }
    printf("k = %f\n", k);
}
    \end{minted}

\end{frame}



\begin{frame}[fragile]
    
    \begin{minted}{zsh}
❯❯ gcc solow.c -o out -lm
❯❯ ./out 

x = 6.240251
    \end{minted}

\end{frame}

\begin{frame}

    Pros

    \begin{itemize}
        \item fast 
    \end{itemize}


    \vspace{0.5em}

    Cons

    \begin{itemize}
        \item time consuming to write
        \item lack of portability
        \item hard to debug
        \item hard to parallelize
        \item low interactivity
    \end{itemize}

\end{frame}

\begin{frame}[fragile]


    For comparison, the same operation in Python:
    
    \begin{minted}{python}

α = 0.4
s = 0.3
δ = 0.1
n = 1_000
k = 0.2

for i in range(n-1):
    k = s * k**α + (1 - δ) * k

print(k)

    \end{minted}

\end{frame}

\begin{frame}

    Pros

    \begin{itemize}
        \item easy to write
        \item high portability
        \item easy to debug
        \item high interactivity
    \end{itemize}

    \vspace{0.5em}

    Cons

    \begin{itemize}
        \item slow
    \end{itemize}

\end{frame}


\begin{frame}
    
    So how can we get 

    \begin{center}
    good execution speeds \navy{and} high productivity / interactivity?
    \end{center}

\end{frame}



\begin{frame}[fragile]
    \frametitle{MATLAB}

    \begin{minted}{matlab}
        A = [2.0, -1.0
             5.0, -0.5];

        b = [0.5, 1.0]';

        x = inv(A) * b
    \end{minted}


\end{frame}



\begin{frame}
    
    \begin{figure}
       \begin{center} % l b r t
        \scalebox{.6}{\includegraphics[trim={2cm 8cm 6cm 3cm},clip]{matlab.pdf}}
       \end{center}
    \end{figure}

    
\end{frame}


\begin{frame}[fragile]
    \frametitle{Python + NumPy}


    \begin{minted}{python}
        import numpy 

        A = ((2.0, -1.0),
             (5.0, -0.5))

        b = (0.5, 1.0)

        A, b = np.array(A), np.array(b)

        x = np.inv(A) @ b
    \end{minted}

\end{frame}

\begin{frame}

    \begin{figure}
       \begin{center} % l b r t
        \scalebox{.6}{\includegraphics[trim={2cm 8cm 6cm 3cm},clip]{numpy.pdf}}
       \end{center}
    \end{figure}


\end{frame}


\begin{frame}[fragile]
    \frametitle{Julia --- rise of the JIT compilers}

    Can do MATLAB / NumPy style vectorized operations

    \begin{minted}{julia}
A = [2.0  -1.0
     5.0  -0.5]

b = [0.5  1.0]'

x = inv(A) * b
    \end{minted}
    
\end{frame}


\begin{frame}
    
    But also has fast loops via an efficient JIT compiler

    \vspace{0.5em}
    \vspace{0.5em}
    \Eg Suppose, again, that we want to compute 
    %
    \begin{equation*}
        k_{t+1} = s k_t^\alpha + (1 - \delta) k_t
    \end{equation*}
    %
    from some given $k_0$ 


    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \begin{itemize}
        \item Iterative, not easily vectorized
    \end{itemize}

\end{frame}


\begin{frame}[fragile]
    
    \begin{minted}{julia}

function solow(k0, α=0.4, δ=0.1, n=1_000)
    k = k0
    for i in 1:(n-1)
        k = s * k^α + (1 - δ) * k
    end
    return k
end

solow(0.2)
    \end{minted}

    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}

    Julia accelerates \texttt{solow} at runtime via a JIT compiler

\end{frame}

\begin{frame}[fragile]
    \frametitle{Python + Numba --- same architecture, same speed}
    
    \begin{minted}{python}
from numba import jit

@jit(nopython=True)
def solow(k0, α=0.4, δ=0.1, n=1_000):
    k = k0
    for i in range(n-1):
        k = s * k**α + (1 - δ) * k
    return k

solow(0.2)
    \end{minted}


    Runs at same speed as Julia / C / Fortran

\end{frame}


\begin{frame}
    \frametitle{Back to JAX}

            \vspace{0.5em}
    \begin{itemize}
        \item \brown{J}ust-in-time compilation
            \vspace{0.5em}
        \item \brown{A}utomatic differentiation
            \vspace{0.5em}
        \item \brown{X}ccelerated linear algebra
    \end{itemize}


\end{frame}



\begin{frame}[fragile]
    \frametitle{Automatic differentiation}
    
    \vspace{-1em}
    \begin{minted}{python}
import jax.numpy as jnp
from jax import grad, jit

def f(θ, x):
  for W, b in θ:
    w = W @ x + b
    x = jnp.tanh(w)  
  return x

def loss(θ, x, y):
  return jnp.sum((y - f(θ, x))**2)

grad_loss = jit(grad(loss))  # Now use gradient descent 
    \end{minted}

\end{frame}



\begin{frame}
    \frametitle{Functional Programming}
    
    JAX adopts a \emp{functional programming style}

    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}
    Key feature: Functions are pure

    \begin{itemize}
        \item Deterministic: same input $\implies$ same output 
        \item Have no side effects (don't modify state outside their scope)
    \end{itemize}

\end{frame}


\begin{frame}[fragile]

    A non-pure function

    \begin{minted}{python}
tax_rate = 0.1  # Global 
price = 10.0    # Global

def add_tax_non_pure():
    global price                
    # The next line both accesses and modifies global state
    price = price * (1 + tax_rate)    
    return price 
    \end{minted}
    
\end{frame}


\begin{frame}[fragile]

    A pure function

    \begin{minted}{python}

def add_tax_non_pure(price, tax_rate=0.1):
    price = price * (1 + tax_rate)    
    return price 
    \end{minted}
    
\end{frame}


\begin{frame}

    General advantages:

    \begin{itemize}
        \item Helps testing: each function can operate in isolation
        \item Data dependencies are explicit, which helps with understanding and optimizing complex computations 
        \item Promotes deterministic behavior and hence reproducibility
        \item Prevents subtle bugs that arise from mutating shared state
    \end{itemize}

\end{frame}



\begin{frame}
    
    Advantages for JAX:

     \begin{itemize}
         \item Functional programming facilitates autodiff because
             pure functions are more straightforward to differentiate (don't mod
             external state
         \item Pure functions are easier to
             parallelize and optimize for hardware accelerators like GPUs (don't
             depend on shared mutable state, more independence)
         \item Transformations can be composed cleanly with multiple
             transformations yielding predictable results
        \item Portability across hardware: The functional approach helps JAX
            create code that can run efficiently across different hardware
            accelerators without requiring hardware-specific implementations.
     \end{itemize}


\end{frame}

\begin{frame}
    \frametitle{JAX PyTrees}

    A PyTree is a concept in the JAX library that refers to a tree-like data structure built from Python containers.

    \Egs

    \begin{itemize}
        \item A dictionary of lists of parameters
        \item A list of dictionaries of parameters, etc.
    \end{itemize}

    JAX can

    \begin{itemize}
        \item apply functions to all leaves in a PyTree structure
        \item differentiate functions with respect to the leaves of PyTrees
        \item etc.
    \end{itemize}

\end{frame}


\begin{frame}
    
    
    \resizebox{1.0\textwidth}{!}{
        \input{pytree_fig}
    }

\end{frame}


\begin{frame}[fragile]
    
    \begin{minted}{python}
# Apply gradient updates to all parameters
def sgd_update(params, grads, learning_rate):
    return jax.tree_map(
        lambda p, g: p - learning_rate * g, 
        params, 
        grads
    )

# Calculate gradients (PyTree with same structure as params)
grads = jax.grad(loss_fn)(params, inputs, targets)

# Update all parameters at once
updated_params = sgd_update(params, grads, 0.01)    
    \end{minted}

\end{frame}


\begin{frame}
    
    Advantages over NumPy / MATLAB

    \vspace{0.5em}
    \begin{itemize}
        \item can specialize machine code based on parameter types / shapes
        \vspace{0.5em}
        \item automatically matches tasks with accelerators (GPU / TPU)
        \vspace{0.5em}
        \item fuses array operations for speed and memory efficiency
    \end{itemize}

\end{frame}

\begin{frame}

    Advantages of JAX (vs PyTorch / Tensorflow / etc.) for economists:
    %
    \begin{itemize}
        \item exposes low level functions
            \vspace{0.5em}
        \item elegant functional programming style -- close to maths
            \vspace{0.5em}
        \item elegant autodiff tools
            \vspace{0.5em}
        \item array operations follow standard NumPy API
            \vspace{0.5em}
        \item automatic parallelization
            \vspace{0.5em}
        \item same code, multiple backends (CPUs, GPUs, TPUs)
    \end{itemize}

\end{frame}

\end{document}
