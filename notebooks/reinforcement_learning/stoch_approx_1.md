---
jupytext:
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.7
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

# Stochastic Approximation for Fixed Points

If you haven't installed the quantecon Python library yet and you want to run
this lecture, you need to execute the following line first:

```{code-cell} ipython3
#!pip install quantecon
```

## Introduction

In this lecture we use stochastic approximation to find fixed points of operators generated by forward looking recursions.

To make the problem concrete, we'll discuss this topic in the context of asset pricing.

So suppose that the value of an asset is given by

$$
    V_t = \mathbb E_t M_{t+1} [V_{t+1} + D_{t+1}]
$$

* $V_t$ is the value at time $t$ (price)
* $D_t$ is dividends 
* $M_t$ is the SDF


Adding Markov structure, we assume that 

* $(X_t)$ is $P$-Markov on finite set $\mathbb X$
* $M_{t+1} = m(X_{t+1})$ for all $t$
* $D_{t+1} = d(X_{t+1})$ for all $t$

We also assume that $r(K) < 1$, where

$$
    (Kf)(x) := \sum_{x'} m(x') f(x')  P(x,x') 
$$

Then the solution has the form $V_t = v(X_t)$ where $v$ solves

$$
    v(x) = \sum_{x'} m(x') [v(x') + d(x')]  P(x,x') 
$$

+++

With $b := Kd$, we can write the equation as 

$$
    v = Tv 
    \quad \text{where} \quad
    Tv := Kv + b
$$

The solution has the form

$$
    v^* = (I - K)^{-1} b
$$

+++

We can also compute $v^*$ by stochastic approximation

Consider the random map $v \mapsto \hat Tv$ given by
the algorithm below

For $x \in \mathbb X$:

1. draw $Y_x \sim P(x, \cdot)$ 
1. set $(\hat Tv)(x) = m(Y_x)[v(Y_x) + d(Y_x)]$

We understand $\hat T$ as an operator from $\mathbb R^n$ into the set of random
vectors in $\mathbb R^n$

+++

We have

$$
\begin{aligned}
    \mathbb E (\hat Tv)(x) 
    & = \sum_{x'} [m(x')v(x') + d(x')] P(x, x')
    \\
    & = (Tv)(x)
\end{aligned}
$$

The SA algorithm is to iterate on 

$$
    v_{k+1} = v_k + \alpha_k (\hat T v_k - v_k)
$$

+++

In the code below we use a standard Lucas tree setting where

$$
m(x') = \beta \exp((1 - \gamma) x')
$$

where 

* $\beta \in (0,1)$ is a discount factor and
* $\gamma \in \mathbb R$ is a nonzero risk aversion parameter

For the dividend process we set $d(x') = 1$ for all $x'$

+++

## Code

```{code-cell} ipython3
import numpy as np
import quantecon as qe
from numba import jit
import matplotlib.pyplot as plt
```

```{code-cell} ipython3
def test_stability(Q):
    """
    Stability test for a given matrix Q.
    """
    sr = np.max(np.abs(np.linalg.eigvals(Q)))
    error_msg = f"Spectral radius condition failed with radius = {sr}"
    assert sr < 1, error_msg

# == fix parameters == #

n = 10
ρ = 0.9
σ = 0.02
β = 0.96
γ = 2.0
mc = qe.tauchen(n, ρ, σ)
P = mc.P
states = mc.state_values

# == solve using linear algebra == #

# Compute the matrix K
K = β * P * np.exp((1 - γ) * states)

# Make sure that a unique solution exists
test_stability(K)

# Compute v
I = np.identity(n)
v_star = np.linalg.solve(I - K, K @ np.ones(n))

# == solve using stochastic approximation == #

P_cdf = np.cumsum(P, axis=1)

@jit
def compute_fixed_point_sa(series_length=1_000_000):
    v = np.zeros(n)
    new_v = np.empty_like(v)
    for k in range(series_length):
        alpha = (k + 1)**(-0.55)
        for i in range(n):
            j = qe.random.draw(P_cdf[i, :])  # an index draw from P[i, :]
            Y = states[j]  # the update state
            hat_Tv = β * np.exp((1 - γ) * Y) * (v[j] + 1)
            new_v[i] = v[i] + alpha * (hat_Tv - v[i])
        error = np.max(np.abs(new_v - v))
        v[:] = new_v
    return v
```

Now let's compute the two solutions and compare them.

```{code-cell} ipython3
v_sa = compute_fixed_point_sa()

fig, ax = plt.subplots()
ax.plot(states, v_star, alpha=0.8, label='linear algebra')
ax.plot(states, v_sa, lw=2, alpha=0.8, ls='dashed', 
        label='stochastic approx')
ax.legend()
plt.show()
```

## A Sequential Version

+++

We have been performing the following update:

At step $k$, for each $x$, we

1. draw $Y_x$ from $P(x, \cdot)$ and 
1. compute

$$
    v_{k+1}(x) = v_k(x) + \alpha_k \{m(Y_x)[v(Y_x) + d(Y_x)] - v_k(x) \}
$$

+++

This suggests another idea:

1. Generate a $P$-Markov sequence $(X_t)$ and
2. compute, at each $t$,


$$
    v_{k+1}(X_t) = v_k(X_t) + \alpha_k \{m(X_{t+1})[v(X_{t+1}) + d(X_{t+1})] - v_k(X_t) \}
$$

Note that $v_{k+1}(y) = v_k(y)$ for all $y \not= X_t$

Intuitively, this should work as long as $X_t$ visits all parts of the state space infinitely often.

+++

Here's a new function that iterates as described above.

```{code-cell} ipython3
@jit
def compute_fixed_point_sequential(series_length=50_000_000):
    v = np.zeros(n)
    i = 0  # index of initial state X_0
    for t in range(series_length):
        alpha = (t + 1)**(-0.55)
        j = qe.random.draw(P_cdf[i, :])  # draw index j of X_{t+1} from P[i, :]
        Y = states[j]  # X_{t+1}
        M = β * np.exp((1 - γ) * Y)
        v[i] = v[i] + alpha * ( M * (v[j] + 1) - v[i])
        i = j
    return v
```

Let's try it out.

```{code-cell} ipython3
v_sa = compute_fixed_point_sequential()

fig, ax = plt.subplots()
ax.plot(states, v_star, alpha=0.8, label='linear algebra')
ax.plot(states, v_sa, lw=2, alpha=0.8, ls='dashed', 
        label='stochastic approx')
ax.legend()
plt.show()
```

```{code-cell} ipython3

```
